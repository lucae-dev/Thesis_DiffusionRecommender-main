{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'warm_user_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TwoRandomWalksSampler\n\u001b[0;32m----> 2\u001b[0m similarity_batch \u001b[38;5;241m=\u001b[39m \u001b[43mTwoRandomWalksSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mURM_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m _ \u001b[38;5;241m=\u001b[39m similarity_batch\u001b[38;5;241m.\u001b[39msample_warm_batch(\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m      4\u001b[0m _\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'warm_user_ids'"
     ]
    }
   ],
   "source": [
    "from Diffusion.similarity_models import TwoRandomWalksSampler\n",
    "similarity_batch = TwoRandomWalksSampler(URM_train)\n",
    "_ = similarity_batch.sample_warm_batch(500)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSplitter_Holdout: Verifying data consistency...\n",
      "DataSplitter_Holdout: Verifying data consistency... Passed!\n",
      "DataSplitter_Holdout: DataReader: Movielens100K\n",
      "\tNum items: 1682\n",
      "\tNum users: 943\n",
      "\tTrain \t\tquota 80.00 (80.00), \tinteractions 80000, \tdensity 5.04E-02\n",
      "\tValidation \tquota 10.00 (10.00), \tinteractions 10000, \tdensity 6.30E-03\n",
      "\tTest \t\tquota 10.00 (10.00), \tinteractions 10000, \tdensity 6.30E-03\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DataSplitter_Holdout: Done.\n",
      "EvaluatorHoldout: Ignoring 27 ( 2.9%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 18 ( 1.9%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Data_manager.DataSplitter_Holdout import DataSplitter_Holdout\n",
    "from Data_manager.Movielens.Movielens100KReader import Movielens100KReader\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "import numpy as np\n",
    "\n",
    "dataset_reader = Movielens100KReader()\n",
    "\n",
    "dataSplitter = DataSplitter_Holdout(dataset_reader, user_wise=False, split_interaction_quota_list=[80, 10, 10])\n",
    "dataSplitter.load_data() #\"results_experiments/Movielens1M/data\"\n",
    "URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "cutoff_list = [10, 50]\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=cutoff_list)\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=cutoff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Diffusion.MultiBlockAttentionDiffusionRecommenderSimilarity import MultiBlockAttentionDiffusionRecommenderInfSimilarity\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    cutoff = 10\n",
    "    metric = 'NDCG'\n",
    "    directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Movielens100K'\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512, 1024])\n",
    "    embeddings_dim = trial.suggest_categorical('embeddings_dim', [64, 128, 256, 512,1024])\n",
    "    heads = trial.suggest_categorical('heads', [1])\n",
    "    attention_blocks = trial.suggest_categorical('attention_blocks', [2, 3, 5, 8, 13])\n",
    "    d_ff = trial.suggest_categorical('d_ff', [512, 1024, 2048])\n",
    "    epochs = trial.suggest_int('epochs', 10, 11)\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    noise_timesteps = trial.suggest_int('noise_timesteps', 1, 2000)\n",
    "    inference_timesteps = trial.suggest_int('inference_timesteps', 1, noise_timesteps-1)\n",
    "    start_beta = trial.suggest_float('start_beta', 0.00001, 0.001)\n",
    "    end_beta = trial.suggest_float('end_beta', 0.01, 0.2)\n",
    "\n",
    "    # Initialize and train the recommender\n",
    "\n",
    "    diffusion_model = MultiBlockAttentionDiffusionRecommenderInfSimilarity(URM_train = URM_train, verbose = False, use_gpu = True)\n",
    "\n",
    "    diffusion_model.fit(\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      embeddings_dim=embeddings_dim,\n",
    "                      heads=heads,\n",
    "                      attention_blocks = attention_blocks,\n",
    "                      d_ff = d_ff,\n",
    "                      l2_reg=l2_reg,\n",
    "                      learning_rate=learning_rate,\n",
    "                      noise_timesteps = noise_timesteps,\n",
    "                      inference_timesteps = inference_timesteps,\n",
    "                      start_beta = start_beta,\n",
    "                      end_beta = end_beta\n",
    "    )\n",
    "\n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "    hyperparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'embeddings_dim': embeddings_dim,\n",
    "    'heads': heads,\n",
    "    'attention_blocks': attention_blocks,\n",
    "    'd_ff': d_ff,\n",
    "    'epochs': epochs,\n",
    "    'l2_reg': l2_reg,\n",
    "    'learning_rate': learning_rate,\n",
    "    'noise_timesteps': noise_timesteps,\n",
    "    'inference_timesteps': inference_timesteps,\n",
    "    'start_beta': start_beta,\n",
    "    'end_beta': end_beta}\n",
    "\n",
    "    result_df['hyperparams'] = str(hyperparams)\n",
    "\n",
    "    filename = directory_path + diffusion_model.RECOMMENDER_NAME + \".csv\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # If it exists, append without writing the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create it, write the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='w', header=True, index=False)\n",
    "\n",
    "    return result_df.loc[cutoff][metric]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MultiBlockAttentionDiffusionRecommenderSimilarity' from 'Diffusion.DiffusionRecommender' (/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/DiffusionRecommender.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDiffusionRecommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiBlockAttentionDiffusionRecommenderSimilarity\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m      9\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MultiBlockAttentionDiffusionRecommenderSimilarity' from 'Diffusion.DiffusionRecommender' (/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/DiffusionRecommender.py)"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommenderSimilarity\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    cutoff = 10\n",
    "    metric = 'NDCG'\n",
    "    directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Movielens100K'\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512]) #Â , 1024]) # Movielens100k has only 943 users!!\n",
    "    embeddings_dim = trial.suggest_categorical('embeddings_dim', [6 ])#128, 256, 512, 1024])\n",
    "    heads = trial.suggest_categorical('heads', [4])\n",
    "    attention_blocks = trial.suggest_categorical('attention_blocks', [1]) #, 2, 3, 5, 8, 13])\n",
    "    d_ff = trial.suggest_categorical('d_ff', [1024, 2048, 4096])\n",
    "    epochs = trial.suggest_int('epochs', 50, 300)\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    noise_timesteps = trial.suggest_int('noise_timesteps', 3, 1000)\n",
    "    inference_timesteps = trial.suggest_int('inference_timesteps', 1, noise_timesteps-1)\n",
    "    start_beta = trial.suggest_float('start_beta', 0.00001, 0.001)\n",
    "    end_beta = trial.suggest_float('end_beta', 0.01, 0.2)\n",
    "\n",
    "    # Initialize and train the recommender\n",
    "\n",
    "    diffusion_model = MultiBlockAttentionDiffusionRecommenderSimilarity(URM_train = URM_train, verbose = False, use_gpu = True)\n",
    "\n",
    "    diffusion_model.fit(\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      embeddings_dim=embeddings_dim,\n",
    "                      heads=heads,\n",
    "                      attention_blocks = attention_blocks,\n",
    "                      d_ff = d_ff,\n",
    "                      l2_reg=l2_reg,\n",
    "                      learning_rate=learning_rate,\n",
    "                      noise_timesteps = noise_timesteps,\n",
    "                      inference_timesteps = inference_timesteps,\n",
    "                      start_beta = start_beta,\n",
    "                      end_beta = end_beta\n",
    "    )\n",
    "\n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "    hyperparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'embeddings_dim': embeddings_dim,\n",
    "    'heads': heads,\n",
    "    'attention_blocks': attention_blocks,\n",
    "    'd_ff': d_ff,\n",
    "    'epochs': epochs,\n",
    "    'l2_reg': l2_reg,\n",
    "    'learning_rate': learning_rate,\n",
    "    'noise_timesteps': noise_timesteps,\n",
    "    'inference_timesteps': inference_timesteps,\n",
    "    'start_beta': start_beta,\n",
    "    'end_beta': end_beta}\n",
    "\n",
    "    result_df['hyperparams'] = str(hyperparams)\n",
    "\n",
    "    filename = directory_path + diffusion_model.RECOMMENDER_NAME + \".csv\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # If it exists, append without writing the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create it, write the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='w', header=True, index=False)\n",
    "\n",
    "    return result_df.loc[cutoff][metric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-11 23:46:06,840] A new study created in memory with name: no-name-f4645319-8190-4756-978d-f60b00b89404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945d46bc594f400f9440c1d0ae8e181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vy/ssvybtks5ms1n3l5ldrh_hv40000gn/T/ipykernel_16810/4095861478.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
      "/var/folders/vy/ssvybtks5ms1n3l5ldrh_hv40000gn/T/ipykernel_16810/4095861478.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/Users/lucaortolomo/miniconda3/envs/Tesi/lib/python3.8/site-packages/scipy/sparse/_index.py:137: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 1 of 11. Elapsed time 0.99 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 2 of 11. Elapsed time 1.50 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 3 of 11. Elapsed time 2.00 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 4 of 11. Elapsed time 2.50 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 5 of 11. Elapsed time 3.00 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 6 of 11. Elapsed time 3.51 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 7 of 11. Elapsed time 4.03 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 8 of 11. Elapsed time 4.54 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 9 of 11. Elapsed time 5.04 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 10 of 11. Elapsed time 5.56 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 11 of 11. Elapsed time 6.07 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Terminating at epoch 11. Elapsed time 6.07 sec\n",
      "[W 2024-03-11 23:46:20,760] Trial 0 failed with parameters: {'batch_size': 64, 'embeddings_dim': 1024, 'heads': 1, 'attention_blocks': 2, 'd_ff': 512, 'epochs': 11, 'l2_reg': 9.582308016398878e-05, 'learning_rate': 0.0001662969635490837, 'noise_timesteps': 1132, 'inference_timesteps': 906, 'start_beta': 0.0006173453740368906, 'end_beta': 0.15002572888041546} because of the following error: TypeError(\"can't assign a numpy.ndarray to a torch.mps.FloatTensor\").\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/lucaortolomo/miniconda3/envs/Tesi/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/vy/ssvybtks5ms1n3l5ldrh_hv40000gn/T/ipykernel_16810/4095861478.py\", line 45, in objective\n",
      "    result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
      "  File \"/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Evaluation/Evaluator.py\", line 276, in evaluateRecommender\n",
      "    results_dict = self._run_evaluation_on_selected_users(recommender_object, self.users_to_evaluate)\n",
      "  File \"/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Evaluation/Evaluator.py\", line 476, in _run_evaluation_on_selected_users\n",
      "    recommended_items_batch_list, scores_batch = recommender_object.recommend(test_user_batch_array,\n",
      "  File \"/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Recommenders/BaseRecommender.py\", line 147, in recommend\n",
      "    scores_batch = self._compute_item_score(user_id_array, items_to_compute=items_to_compute)\n",
      "  File \"/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/MultiBlockAttentionDiffusionRecommenderSimilarity.py\", line 355, in _compute_item_score\n",
      "    user_profile_inference[i] = self.diffusion_model.inference(user_batch_tensor, self.inference_timesteps)[0]\n",
      "TypeError: can't assign a numpy.ndarray to a torch.mps.FloatTensor\n",
      "[W 2024-03-11 23:46:20,762] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't assign a numpy.ndarray to a torch.mps.FloatTensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     28\u001b[0m diffusion_model \u001b[38;5;241m=\u001b[39m MultiBlockAttentionDiffusionRecommenderInfSimilarity(URM_train \u001b[38;5;241m=\u001b[39m URM_train, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, use_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m diffusion_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     31\u001b[0m                   epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     32\u001b[0m                   batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m                   end_beta \u001b[38;5;241m=\u001b[39m end_beta\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 45\u001b[0m result_df, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator_validation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluateRecommender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: batch_size,\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_dim\u001b[39m\u001b[38;5;124m'\u001b[39m: embeddings_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_beta\u001b[39m\u001b[38;5;124m'\u001b[39m: start_beta,\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_beta\u001b[39m\u001b[38;5;124m'\u001b[39m: end_beta}\n\u001b[1;32m     60\u001b[0m result_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparams\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(hyperparams)\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Evaluation/Evaluator.py:276\u001b[0m, in \u001b[0;36mEvaluator.evaluateRecommender\u001b[0;34m(self, recommender_object)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time_print \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_users_evaluated \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 276\u001b[0m results_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_evaluation_on_selected_users\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecommender_object\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musers_to_evaluate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_users_evaluated \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cutoff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcutoff_list:\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Evaluation/Evaluator.py:476\u001b[0m, in \u001b[0;36mEvaluatorHoldout._run_evaluation_on_selected_users\u001b[0;34m(self, recommender_object, users_to_evaluate, block_size)\u001b[0m\n\u001b[1;32m    473\u001b[0m     user_batch_start \u001b[38;5;241m=\u001b[39m user_batch_end\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# Compute predictions for a batch of users using vectorization, much more efficient than computing it one at a time\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m     recommended_items_batch_list, scores_batch \u001b[38;5;241m=\u001b[39m \u001b[43mrecommender_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecommend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_user_batch_array\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mremove_seen_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_seen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_cutoff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mremove_top_pop_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mremove_custom_items_flag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_items_flag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m                                                              \u001b[49m\u001b[43mreturn_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    482\u001b[0m \u001b[43m                                                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     results_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_metrics_on_recommendation_list(test_user_batch_array \u001b[38;5;241m=\u001b[39m test_user_batch_array,\n\u001b[1;32m    485\u001b[0m                                                  recommended_items_batch_list \u001b[38;5;241m=\u001b[39m recommended_items_batch_list,\n\u001b[1;32m    486\u001b[0m                                                  scores_batch \u001b[38;5;241m=\u001b[39m scores_batch,\n\u001b[1;32m    487\u001b[0m                                                  results_dict \u001b[38;5;241m=\u001b[39m results_dict)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results_dict\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Recommenders/BaseRecommender.py:147\u001b[0m, in \u001b[0;36mBaseRecommender.recommend\u001b[0;34m(self, user_id_array, cutoff, remove_seen_flag, items_to_compute, remove_top_pop_flag, remove_custom_items_flag, return_scores)\u001b[0m\n\u001b[1;32m    143\u001b[0m cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(cutoff, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mURM_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# Compute the scores using the model-specific function\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Vectorize over all users in user_id_array\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m scores_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_item_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_id_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems_to_compute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mitems_to_compute\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m user_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(user_id_array)):\n\u001b[1;32m    152\u001b[0m     user_id \u001b[38;5;241m=\u001b[39m user_id_array[user_index]\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/MultiBlockAttentionDiffusionRecommenderSimilarity.py:355\u001b[0m, in \u001b[0;36mMultiBlockAttentionDiffusionRecommenderInfSimilarity._compute_item_score\u001b[0;34m(self, user_id_array, items_to_compute)\u001b[0m\n\u001b[1;32m    352\u001b[0m         user_batch_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(user_batch_tensor, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# Perform inference and store the result directly in the preallocated tensor\u001b[39;00m\n\u001b[0;32m--> 355\u001b[0m     user_profile_inference[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_model\u001b[38;5;241m.\u001b[39minference(user_batch_tensor, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_timesteps)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(user_id_array))\n\u001b[1;32m    358\u001b[0m user_profile_inference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(user_profile_inference, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't assign a numpy.ndarray to a torch.mps.FloatTensor"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBlockAttentionDiffusionRecommender: URM Detected 26 ( 1.5%) items with no interactions.\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 1, loss 6.97E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 1 of 5. Elapsed time 4.19 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 2, loss 6.17E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 2 of 5. Elapsed time 4.38 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 3, loss 6.28E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 3 of 5. Elapsed time 4.57 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 4, loss 6.13E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 4 of 5. Elapsed time 4.75 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 5, loss 6.20E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 5 of 5. Elapsed time 4.93 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Terminating at epoch 5. Elapsed time 4.94 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Training complete\n"
     ]
    }
   ],
   "source": [
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommender\n",
    "diffusion_model = MultiBlockAttentionDiffusionRecommender(URM_train = URM_train, use_gpu = True)\n",
    "diffusion_model.fit(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Processed 916 (100.0%) in 2.60 sec. Users per second: 353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.095306</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.109041</td>\n",
       "      <td>0.042878</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.258577</td>\n",
       "      <td>0.117581</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.56441</td>\n",
       "      <td>0.330026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.54825</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>4.558766</td>\n",
       "      <td>0.952265</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.469742</td>\n",
       "      <td>2.290069</td>\n",
       "      <td>0.095436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.276977</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>0.01491</td>\n",
       "      <td>0.066893</td>\n",
       "      <td>0.271178</td>\n",
       "      <td>0.180707</td>\n",
       "      <td>0.094046</td>\n",
       "      <td>0.796943</td>\n",
       "      <td>0.411937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.774125</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.045353</td>\n",
       "      <td>6.563823</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.676347</td>\n",
       "      <td>1.652778</td>\n",
       "      <td>0.507614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.095306                    0.141  0.109041  0.042878    0.062498   \n",
       "50      0.056659                 0.276977  0.276483   0.01491    0.066893   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.258577  0.117581  0.101712   0.56441      0.330026  ...   \n",
       "50      0.271178  0.180707  0.094046  0.796943      0.411937  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.971368           0.54825    0.971368       0.011682   \n",
       "50          0.971368          0.774125    0.971368       0.045353   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10            4.558766                   0.952265             0.031481   \n",
       "50            6.563823                   0.989559             0.122222   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                  0.469742                 2.290069      0.095436  \n",
       "50                  0.676347                 1.652778      0.507614  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSplitter_Holdout: Verifying data consistency...\n",
      "DataSplitter_Holdout: Verifying data consistency... Passed!\n",
      "DataSplitter_Holdout: DataReader: CiteULike_a\n",
      "\tNum items: 15429\n",
      "\tNum users: 5536\n",
      "\tTrain \t\tquota 80.00 (80.00), \tinteractions 160144, \tdensity 1.87E-03\n",
      "\tValidation \tquota 10.00 (10.00), \tinteractions 20018, \tdensity 2.34E-04\n",
      "\tTest \t\tquota 10.00 (10.00), \tinteractions 20018, \tdensity 2.34E-04\n",
      "\n",
      "\n",
      "\n",
      "\tICM name: ICM_title_abstract, Num features: 7999, feature occurrences: 1031068, density 8.35E-03\n",
      "\n",
      "\n",
      "DataSplitter_Holdout: Done.\n",
      "EvaluatorHoldout: Ignoring 795 (14.4%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 742 (13.4%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Data_manager.DataSplitter_Holdout import DataSplitter_Holdout\n",
    "from Data_manager.CiteULike.CiteULikeReader import CiteULike_aReader\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "import numpy as np\n",
    "\n",
    "dataset_reader = CiteULike_aReader()\n",
    "\n",
    "dataSplitter = DataSplitter_Holdout(dataset_reader, user_wise=False, split_interaction_quota_list=[80, 10, 10])\n",
    "dataSplitter.load_data('/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Hyperparameter_databases/hyperparameter_database_2024_02/k_5_cores/original/hyperopt_random_holdout_80_10_10/CiteULike_a/data') #\"results_experiments/Movielens1M/data\"\n",
    "URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "cutoff_list = [10, 50]\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=cutoff_list)\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=cutoff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Dataset/full/ created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Dataset/' + \"full\" + '/' \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory {directory_path} created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucaortolomo/miniconda3/envs/Tesi/lib/python3.8/site-packages/scipy/sparse/_index.py:137: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Diffusion.similarity_models import TwoRandomWalksSampler\n",
    "\n",
    "user = 200\n",
    "sampler = TwoRandomWalksSampler(URM=URM_train, warm_user_ids=None)\n",
    "user_batch = sampler.sample_batch(256, user)\n",
    "user_batch_tensor = URM_train[user_batch]\n",
    "user_profile_reference = URM_train[user].toarray()\n",
    "\n",
    "# Convert CSR matrix to a dense numpy array directly\n",
    "user_batch_dense_np = user_batch_tensor.toarray()\n",
    "\n",
    "# Convert the dense numpy array to a PyTorch tensor\n",
    "# and move it to the appropriate device\n",
    "if str('mps') == 'mps':\n",
    "    user_batch_tensor = torch.tensor(user_batch_dense_np, dtype=torch.float32, device='cpu').to('mps')\n",
    "else:\n",
    "# Transferring only the sparse structure to reduce the data transfer\n",
    "    user_batch_tensor = torch.sparse_csr_tensor(user_batch_tensor.indptr,\n",
    "                                                user_batch_tensor.indices,\n",
    "                                                user_batch_tensor.data,\n",
    "                                                size=user_batch_tensor.shape,\n",
    "                                                dtype=torch.float32,\n",
    "                                                device='cpu',\n",
    "                                                requires_grad=False).to_dense()\n",
    "    \n",
    "print(user_batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
