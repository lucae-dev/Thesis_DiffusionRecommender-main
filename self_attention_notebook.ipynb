{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'warm_user_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TwoRandomWalksSampler\n\u001b[0;32m----> 2\u001b[0m similarity_batch \u001b[38;5;241m=\u001b[39m \u001b[43mTwoRandomWalksSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mURM_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m _ \u001b[38;5;241m=\u001b[39m similarity_batch\u001b[38;5;241m.\u001b[39msample_warm_batch(\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m      4\u001b[0m _\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'warm_user_ids'"
     ]
    }
   ],
   "source": [
    "from Diffusion.similarity_models import TwoRandomWalksSampler\n",
    "similarity_batch = TwoRandomWalksSampler(URM_train)\n",
    "_ = similarity_batch.sample_warm_batch(500)\n",
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSplitter_Holdout: Verifying data consistency...\n",
      "DataSplitter_Holdout: Verifying data consistency... Passed!\n",
      "DataSplitter_Holdout: DataReader: Movielens100K\n",
      "\tNum items: 1682\n",
      "\tNum users: 943\n",
      "\tTrain \t\tquota 80.00 (80.00), \tinteractions 80000, \tdensity 5.04E-02\n",
      "\tValidation \tquota 10.00 (10.00), \tinteractions 10000, \tdensity 6.30E-03\n",
      "\tTest \t\tquota 10.00 (10.00), \tinteractions 10000, \tdensity 6.30E-03\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DataSplitter_Holdout: Done.\n",
      "EvaluatorHoldout: Ignoring 27 ( 2.9%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 18 ( 1.9%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Data_manager.DataSplitter_Holdout import DataSplitter_Holdout\n",
    "from Data_manager.Movielens.Movielens100KReader import Movielens100KReader\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "import numpy as np\n",
    "\n",
    "dataset_reader = Movielens100KReader()\n",
    "\n",
    "dataSplitter = DataSplitter_Holdout(dataset_reader, user_wise=False, split_interaction_quota_list=[80, 10, 10])\n",
    "dataSplitter.load_data() #\"results_experiments/Movielens1M/data\"\n",
    "URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "cutoff_list = [10, 50]\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=cutoff_list)\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=cutoff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Diffusion.MultiBlockAttentionDiffusionRecommenderSimilarity import MultiBlockAttentionDiffusionRecommenderInfSimilarity\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    cutoff = 10\n",
    "    metric = 'NDCG'\n",
    "    directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Movielens100K'\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512])\n",
    "    embeddings_dim = trial.suggest_categorical('embeddings_dim', [64, 128, 256, 512,1024])\n",
    "    heads = trial.suggest_categorical('heads', [1])\n",
    "    attention_blocks = trial.suggest_categorical('attention_blocks', [2, 3, 5, 8, 13])\n",
    "    d_ff = trial.suggest_categorical('d_ff', [512, 1024, 2048])\n",
    "    epochs = trial.suggest_int('epochs', 1, 2)\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    noise_timesteps = trial.suggest_int('noise_timesteps', 1, 2000)\n",
    "    inference_timesteps = trial.suggest_int('inference_timesteps', 1, noise_timesteps-1)\n",
    "    start_beta = trial.suggest_float('start_beta', 0.00001, 0.001)\n",
    "    end_beta = trial.suggest_float('end_beta', 0.01, 0.2)\n",
    "\n",
    "    # Initialize and train the recommender\n",
    "\n",
    "    diffusion_model = MultiBlockAttentionDiffusionRecommenderInfSimilarity(URM_train = URM_train, verbose = False, use_gpu = True)\n",
    "\n",
    "    diffusion_model.fit(\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      embeddings_dim=embeddings_dim,\n",
    "                      heads=heads,\n",
    "                      attention_blocks = attention_blocks,\n",
    "                      d_ff = d_ff,\n",
    "                      l2_reg=l2_reg,\n",
    "                      learning_rate=learning_rate,\n",
    "                      noise_timesteps = noise_timesteps,\n",
    "                      inference_timesteps = inference_timesteps,\n",
    "                      start_beta = start_beta,\n",
    "                      end_beta = end_beta\n",
    "    )\n",
    "\n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "    hyperparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'embeddings_dim': embeddings_dim,\n",
    "    'heads': heads,\n",
    "    'attention_blocks': attention_blocks,\n",
    "    'd_ff': d_ff,\n",
    "    'epochs': epochs,\n",
    "    'l2_reg': l2_reg,\n",
    "    'learning_rate': learning_rate,\n",
    "    'noise_timesteps': noise_timesteps,\n",
    "    'inference_timesteps': inference_timesteps,\n",
    "    'start_beta': start_beta,\n",
    "    'end_beta': end_beta}\n",
    "\n",
    "    result_df['hyperparams'] = str(hyperparams)\n",
    "\n",
    "    filename = directory_path + diffusion_model.RECOMMENDER_NAME + \".csv\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # If it exists, append without writing the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create it, write the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='w', header=True, index=False)\n",
    "\n",
    "    return result_df.loc[cutoff][metric]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MultiBlockAttentionDiffusionRecommenderSimilarity' from 'Diffusion.DiffusionRecommender' (/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/DiffusionRecommender.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDiffusionRecommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiBlockAttentionDiffusionRecommenderSimilarity\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[1;32m      9\u001b[0m     cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MultiBlockAttentionDiffusionRecommenderSimilarity' from 'Diffusion.DiffusionRecommender' (/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/DiffusionRecommender.py)"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommenderSimilarity\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    cutoff = 10\n",
    "    metric = 'NDCG'\n",
    "    directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Movielens100K'\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512]) # , 1024]) # Movielens100k has only 943 users!!\n",
    "    embeddings_dim = trial.suggest_categorical('embeddings_dim', [6 ])#128, 256, 512, 1024])\n",
    "    heads = trial.suggest_categorical('heads', [4])\n",
    "    attention_blocks = trial.suggest_categorical('attention_blocks', [1]) #, 2, 3, 5, 8, 13])\n",
    "    d_ff = trial.suggest_categorical('d_ff', [1024, 2048, 4096])\n",
    "    epochs = trial.suggest_int('epochs', 50, 300)\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    noise_timesteps = trial.suggest_int('noise_timesteps', 3, 1000)\n",
    "    inference_timesteps = trial.suggest_int('inference_timesteps', 1, noise_timesteps-1)\n",
    "    start_beta = trial.suggest_float('start_beta', 0.00001, 0.001)\n",
    "    end_beta = trial.suggest_float('end_beta', 0.01, 0.2)\n",
    "\n",
    "    # Initialize and train the recommender\n",
    "\n",
    "    diffusion_model = MultiBlockAttentionDiffusionRecommenderSimilarity(URM_train = URM_train, verbose = False, use_gpu = True)\n",
    "\n",
    "    diffusion_model.fit(\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      embeddings_dim=embeddings_dim,\n",
    "                      heads=heads,\n",
    "                      attention_blocks = attention_blocks,\n",
    "                      d_ff = d_ff,\n",
    "                      l2_reg=l2_reg,\n",
    "                      learning_rate=learning_rate,\n",
    "                      noise_timesteps = noise_timesteps,\n",
    "                      inference_timesteps = inference_timesteps,\n",
    "                      start_beta = start_beta,\n",
    "                      end_beta = end_beta\n",
    "    )\n",
    "\n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "    hyperparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'embeddings_dim': embeddings_dim,\n",
    "    'heads': heads,\n",
    "    'attention_blocks': attention_blocks,\n",
    "    'd_ff': d_ff,\n",
    "    'epochs': epochs,\n",
    "    'l2_reg': l2_reg,\n",
    "    'learning_rate': learning_rate,\n",
    "    'noise_timesteps': noise_timesteps,\n",
    "    'inference_timesteps': inference_timesteps,\n",
    "    'start_beta': start_beta,\n",
    "    'end_beta': end_beta}\n",
    "\n",
    "    result_df['hyperparams'] = str(hyperparams)\n",
    "\n",
    "    filename = directory_path + diffusion_model.RECOMMENDER_NAME + \".csv\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # If it exists, append without writing the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create it, write the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='w', header=True, index=False)\n",
    "\n",
    "    return result_df.loc[cutoff][metric]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-13 08:38:29,812] A new study created in memory with name: no-name-bda10161-2b01-4534-a68b-e533935fbfac\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d940383d6dc149258001c96dd300fcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vy/ssvybtks5ms1n3l5ldrh_hv40000gn/T/ipykernel_22906/2648829577.py:19: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
      "/var/folders/vy/ssvybtks5ms1n3l5ldrh_hv40000gn/T/ipykernel_22906/2648829577.py:20: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
      "/Users/lucaortolomo/miniconda3/envs/Tesi/lib/python3.8/site-packages/scipy/sparse/_index.py:137: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Epoch 1 of 1. Elapsed time 0.97 sec\n",
      "MultiBlockAttentionDiffusionRecommenderSimilarity: Terminating at epoch 1. Elapsed time 0.98 sec\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=1,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=200,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiBlockAttentionDiffusionRecommender: URM Detected 26 ( 1.5%) items with no interactions.\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 1, loss 6.97E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 1 of 5. Elapsed time 4.19 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 2, loss 6.17E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 2 of 5. Elapsed time 4.38 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 3, loss 6.28E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 3 of 5. Elapsed time 4.57 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 4, loss 6.13E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 4 of 5. Elapsed time 4.75 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 5, loss 6.20E+00\n",
      "MultiBlockAttentionDiffusionRecommender: Epoch 5 of 5. Elapsed time 4.93 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Terminating at epoch 5. Elapsed time 4.94 sec\n",
      "MultiBlockAttentionDiffusionRecommender: Training complete\n"
     ]
    }
   ],
   "source": [
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommender\n",
    "diffusion_model = MultiBlockAttentionDiffusionRecommender(URM_train = URM_train, use_gpu = True)\n",
    "diffusion_model.fit(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvaluatorHoldout: Processed 916 (100.0%) in 2.60 sec. Users per second: 353\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRECISION</th>\n",
       "      <th>PRECISION_RECALL_MIN_DEN</th>\n",
       "      <th>RECALL</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAP_MIN_DEN</th>\n",
       "      <th>MRR</th>\n",
       "      <th>NDCG</th>\n",
       "      <th>F1</th>\n",
       "      <th>HIT_RATE</th>\n",
       "      <th>ARHR_ALL_HITS</th>\n",
       "      <th>...</th>\n",
       "      <th>COVERAGE_USER</th>\n",
       "      <th>COVERAGE_USER_HIT</th>\n",
       "      <th>USERS_IN_GT</th>\n",
       "      <th>DIVERSITY_GINI</th>\n",
       "      <th>SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_DIVERSITY_HERFINDAHL</th>\n",
       "      <th>RATIO_DIVERSITY_GINI</th>\n",
       "      <th>RATIO_SHANNON_ENTROPY</th>\n",
       "      <th>RATIO_AVERAGE_POPULARITY</th>\n",
       "      <th>RATIO_NOVELTY</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cutoff</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.095306</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.109041</td>\n",
       "      <td>0.042878</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.258577</td>\n",
       "      <td>0.117581</td>\n",
       "      <td>0.101712</td>\n",
       "      <td>0.56441</td>\n",
       "      <td>0.330026</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.54825</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>4.558766</td>\n",
       "      <td>0.952265</td>\n",
       "      <td>0.031481</td>\n",
       "      <td>0.469742</td>\n",
       "      <td>2.290069</td>\n",
       "      <td>0.095436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.056659</td>\n",
       "      <td>0.276977</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>0.01491</td>\n",
       "      <td>0.066893</td>\n",
       "      <td>0.271178</td>\n",
       "      <td>0.180707</td>\n",
       "      <td>0.094046</td>\n",
       "      <td>0.796943</td>\n",
       "      <td>0.411937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.774125</td>\n",
       "      <td>0.971368</td>\n",
       "      <td>0.045353</td>\n",
       "      <td>6.563823</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.122222</td>\n",
       "      <td>0.676347</td>\n",
       "      <td>1.652778</td>\n",
       "      <td>0.507614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       PRECISION PRECISION_RECALL_MIN_DEN    RECALL       MAP MAP_MIN_DEN  \\\n",
       "cutoff                                                                      \n",
       "10      0.095306                    0.141  0.109041  0.042878    0.062498   \n",
       "50      0.056659                 0.276977  0.276483   0.01491    0.066893   \n",
       "\n",
       "             MRR      NDCG        F1  HIT_RATE ARHR_ALL_HITS  ...  \\\n",
       "cutoff                                                        ...   \n",
       "10      0.258577  0.117581  0.101712   0.56441      0.330026  ...   \n",
       "50      0.271178  0.180707  0.094046  0.796943      0.411937  ...   \n",
       "\n",
       "       COVERAGE_USER COVERAGE_USER_HIT USERS_IN_GT DIVERSITY_GINI  \\\n",
       "cutoff                                                              \n",
       "10          0.971368           0.54825    0.971368       0.011682   \n",
       "50          0.971368          0.774125    0.971368       0.045353   \n",
       "\n",
       "       SHANNON_ENTROPY RATIO_DIVERSITY_HERFINDAHL RATIO_DIVERSITY_GINI  \\\n",
       "cutoff                                                                   \n",
       "10            4.558766                   0.952265             0.031481   \n",
       "50            6.563823                   0.989559             0.122222   \n",
       "\n",
       "       RATIO_SHANNON_ENTROPY RATIO_AVERAGE_POPULARITY RATIO_NOVELTY  \n",
       "cutoff                                                               \n",
       "10                  0.469742                 2.290069      0.095436  \n",
       "50                  0.676347                 1.652778      0.507614  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataSplitter_Holdout: Verifying data consistency...\n",
      "DataSplitter_Holdout: Verifying data consistency... Passed!\n",
      "DataSplitter_Holdout: DataReader: CiteULike_a\n",
      "\tNum items: 15429\n",
      "\tNum users: 5536\n",
      "\tTrain \t\tquota 80.00 (80.00), \tinteractions 160144, \tdensity 1.87E-03\n",
      "\tValidation \tquota 10.00 (10.00), \tinteractions 20018, \tdensity 2.34E-04\n",
      "\tTest \t\tquota 10.00 (10.00), \tinteractions 20018, \tdensity 2.34E-04\n",
      "\n",
      "\n",
      "\n",
      "\tICM name: ICM_title_abstract, Num features: 7999, feature occurrences: 1031068, density 8.35E-03\n",
      "\n",
      "\n",
      "DataSplitter_Holdout: Done.\n",
      "EvaluatorHoldout: Ignoring 795 (14.4%) Users that have less than 1 test interactions\n",
      "EvaluatorHoldout: Ignoring 742 (13.4%) Users that have less than 1 test interactions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from Data_manager.DataSplitter_Holdout import DataSplitter_Holdout\n",
    "from Data_manager.CiteULike.CiteULikeReader import CiteULike_aReader\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "import numpy as np\n",
    "\n",
    "dataset_reader = CiteULike_aReader()\n",
    "\n",
    "dataSplitter = DataSplitter_Holdout(dataset_reader, user_wise=False, split_interaction_quota_list=[80, 10, 10])\n",
    "dataSplitter.load_data('/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Hyperparameter_databases/hyperparameter_database_2024_02/k_5_cores/original/hyperopt_random_holdout_80_10_10/CiteULike_a/data') #\"results_experiments/Movielens1M/data\"\n",
    "URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "cutoff_list = [10, 50]\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=cutoff_list)\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=cutoff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Dataset/full/ created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Dataset/' + \"full\" + '/' \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "    print(f\"Directory {directory_path} created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucaortolomo/miniconda3/envs/Tesi/lib/python3.8/site-packages/scipy/sparse/_index.py:137: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray_sparse(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [3., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from Diffusion.similarity_models import TwoRandomWalksSampler\n",
    "\n",
    "user = 200\n",
    "sampler = TwoRandomWalksSampler(URM=URM_train, warm_user_ids=None)\n",
    "user_batch = sampler.sample_batch(256, user)\n",
    "user_batch_tensor = URM_train[user_batch]\n",
    "user_profile_reference = URM_train[user].toarray()\n",
    "\n",
    "# Convert CSR matrix to a dense numpy array directly\n",
    "user_batch_dense_np = user_batch_tensor.toarray()\n",
    "\n",
    "# Convert the dense numpy array to a PyTorch tensor\n",
    "# and move it to the appropriate device\n",
    "if str('mps') == 'mps':\n",
    "    user_batch_tensor = torch.tensor(user_batch_dense_np, dtype=torch.float32, device='cpu').to('mps')\n",
    "else:\n",
    "# Transferring only the sparse structure to reduce the data transfer\n",
    "    user_batch_tensor = torch.sparse_csr_tensor(user_batch_tensor.indptr,\n",
    "                                                user_batch_tensor.indices,\n",
    "                                                user_batch_tensor.data,\n",
    "                                                size=user_batch_tensor.shape,\n",
    "                                                dtype=torch.float32,\n",
    "                                                device='cpu',\n",
    "                                                requires_grad=False).to_dense()\n",
    "    \n",
    "print(user_batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
