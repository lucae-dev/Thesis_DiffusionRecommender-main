{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.sparse as sps\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile, os\n",
    "# If file exists, skip the download\n",
    "data_file_path = \"Data_manager_split_datasets/Movielens10M/\"\n",
    "data_file_name = data_file_path + \"movielens_10m.zip\"\n",
    "\n",
    "# If directory does not exist, create\n",
    "if not os.path.exists(data_file_path):\n",
    "    os.makedirs(data_file_path)\n",
    "\n",
    "if not os.path.exists(data_file_name):\n",
    "    urlretrieve (\"http://files.grouplens.org/datasets/movielens/ml-10m.zip\", data_file_name)\n",
    "dataFile = zipfile.ZipFile(data_file_path + \"movielens_10m.zip\")\n",
    "\n",
    "URM_path = dataFile.extract(\"ml-10M100K/ratings.dat\", path = data_file_path + \"decompressed/\")\n",
    "\n",
    "URM_file = open(URM_path, 'r')\n",
    "URM_all_dataframe = pd.read_csv(filepath_or_buffer=URM_path, \n",
    "                                sep=\"::\", \n",
    "                                header=None, \n",
    "                                dtype={0:int, 1:int, 2:float, 3:int},\n",
    "                                engine='python')\n",
    "\n",
    "URM_all_dataframe.columns = [\"UserID\", \"ItemID\", \"Interaction\", \"Timestamp\"]\n",
    "userID_unique = URM_all_dataframe[\"UserID\"].unique()\n",
    "itemID_unique = URM_all_dataframe[\"ItemID\"].unique()\n",
    "n_users = len(userID_unique)\n",
    "n_items = len(itemID_unique)\n",
    "n_interactions = len(URM_all_dataframe)\n",
    "\n",
    "print (\"Number of items\\t {}, Number of users\\t {}\".format(n_items, n_users))\n",
    "print (\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemID_unique), max(userID_unique)))\n",
    "mapped_id, original_id = pd.factorize(URM_all_dataframe[\"UserID\"].unique())\n",
    "user_original_ID_to_index = pd.Series(mapped_id, index=original_id)\n",
    "\n",
    "mapped_id, original_id = pd.factorize(URM_all_dataframe[\"ItemID\"].unique())\n",
    "item_original_ID_to_index = pd.Series(mapped_id, index=original_id)\n",
    "URM_all_dataframe[\"UserID\"] = URM_all_dataframe[\"UserID\"].map(user_original_ID_to_index)\n",
    "URM_all_dataframe[\"ItemID\"] = URM_all_dataframe[\"ItemID\"].map(item_original_ID_to_index)\n",
    "userID_unique = URM_all_dataframe[\"UserID\"].unique()\n",
    "itemID_unique = URM_all_dataframe[\"ItemID\"].unique()\n",
    "\n",
    "n_users = len(userID_unique)\n",
    "n_items = len(itemID_unique)\n",
    "n_interactions = len(URM_all_dataframe)\n",
    "\n",
    "print (\"Number of items\\t {}, Number of users\\t {}\".format(n_items, n_users))\n",
    "print (\"Max ID items\\t {}, Max Id users\\t {}\\n\".format(max(itemID_unique), max(userID_unique)))\n",
    "print (\"Average interactions per user {:.2f}\".format(n_interactions/n_users))\n",
    "print (\"Average interactions per item {:.2f}\\n\".format(n_interactions/n_items))\n",
    "\n",
    "print (\"Sparsity {:.2f} %\".format((1-float(n_interactions)/(n_items*n_users))*100))\n",
    "\n",
    "URM_all = sps.coo_matrix((URM_all_dataframe[\"Interaction\"].values, \n",
    "                          (URM_all_dataframe[\"UserID\"].values, URM_all_dataframe[\"ItemID\"].values)))\n",
    "\n",
    "URM_all.tocsr()\n",
    "train_test_split = 0.80\n",
    "\n",
    "n_interactions = URM_all.nnz\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TwoRandomWalksSimilarity' from 'Diffusion.similarity_models' (/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/similarity_models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDiffusionRecommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiBlockAttentionDiffusionRecommender\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/DiffusionRecommender.py:10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mCreated on 29/09/2022\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m@author: Maurizio Ferrari Dacrema\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mAttentionModels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleAttentionDiffusionModel\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mDiffusion\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TwoRandomWalksSimilarity\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRecommenders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mBaseRecommender\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseRecommender\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRecommenders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mIncremental_Training_Early_Stopping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Incremental_Training_Early_Stopping\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TwoRandomWalksSimilarity' from 'Diffusion.similarity_models' (/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/similarity_models.py)"
     ]
    }
   ],
   "source": [
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "\n",
    "URM_train, URM_test = split_train_in_two_percentage_global_sample(URM_all, train_percentage = 0.80)\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample(URM_train, train_percentage = 0.80)\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Data_manager.DataSplitter_Holdout import DataSplitter_Holdout\n",
    "from Data_manager.Movielens.Movielens100KReader import Movielens100KReader\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "import numpy as np\n",
    "\n",
    "dataset_reader = Movielens100KReader()\n",
    "\n",
    "dataSplitter = DataSplitter_Holdout(dataset_reader, user_wise=False, split_interaction_quota_list=[80, 10, 10])\n",
    "dataSplitter.load_data() #\"results_experiments/Movielens1M/data\"\n",
    "URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "cutoff_list = [10, 50]\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=cutoff_list)\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=cutoff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Data_manager.DataSplitter_Holdout import DataSplitter_Holdout\n",
    "from Data_manager.Movielens.Movielens1MReader import Movielens1MReader\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "import numpy as np\n",
    "\n",
    "dataset_reader = Movielens1MReader()\n",
    "\n",
    "dataSplitter = DataSplitter_Holdout(dataset_reader, user_wise=False, split_interaction_quota_list=[80, 10, 10])\n",
    "dataSplitter.load_data('/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/teams/Movielens1M/data') #\"results_experiments/Movielens1M/data\"\n",
    "URM_train, URM_validation, URM_test = dataSplitter.get_holdout_split()\n",
    "\n",
    "cutoff_list = [10, 50]\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=cutoff_list)\n",
    "evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=cutoff_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommender\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    cutoff = 10\n",
    "    metric = 'NDCG'\n",
    "    directory_path = '/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Self-Attention/OptunaResults/Movielens100K'\n",
    "\n",
    "    batch_size = trial.suggest_categorical('batch_size', [64, 128, 256, 512, 1024])\n",
    "    embeddings_dim = trial.suggest_categorical('embeddings_dim', [64, 128, 256, 512,1024])\n",
    "    heads = trial.suggest_categorical('heads', [1])\n",
    "    attention_blocks = trial.suggest_categorical('attention_blocks', [2, 3, 5, 8, 13])\n",
    "    d_ff = trial.suggest_categorical('d_ff', [512, 1024, 2048])\n",
    "    epochs = trial.suggest_int('epochs', 200, 600)\n",
    "    l2_reg = trial.suggest_loguniform('l2_reg', 1e-5, 1e-3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 1e-2)\n",
    "    noise_timesteps = trial.suggest_int('noise_timesteps', 1, 2000)\n",
    "    inference_timesteps = trial.suggest_int('inference_timesteps', 1, noise_timesteps-1)\n",
    "    start_beta = trial.suggest_float('start_beta', 0.00001, 0.001)\n",
    "    end_beta = trial.suggest_float('end_beta', 0.01, 0.2)\n",
    "\n",
    "    # Initialize and train the recommender\n",
    "\n",
    "    diffusion_model = MultiBlockAttentionDiffusionRecommender(URM_train = URM_train, verbose = False, use_gpu = True)\n",
    "\n",
    "    diffusion_model.fit(\n",
    "                      epochs=epochs,\n",
    "                      batch_size=batch_size,\n",
    "                      embeddings_dim=embeddings_dim,\n",
    "                      heads=heads,\n",
    "                      attention_blocks = attention_blocks,\n",
    "                      d_ff = d_ff,\n",
    "                      l2_reg=l2_reg,\n",
    "                      learning_rate=learning_rate,\n",
    "                      noise_timesteps = noise_timesteps,\n",
    "                      inference_timesteps = inference_timesteps,\n",
    "                      start_beta = start_beta,\n",
    "                      end_beta = end_beta\n",
    "    )\n",
    "\n",
    "    result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "    hyperparams = {\n",
    "    'batch_size': batch_size,\n",
    "    'embeddings_dim': embeddings_dim,\n",
    "    'heads': heads,\n",
    "    'attention_blocks': attention_blocks,\n",
    "    'd_ff': d_ff,\n",
    "    'epochs': epochs,\n",
    "    'l2_reg': l2_reg,\n",
    "    'learning_rate': learning_rate,\n",
    "    'noise_timesteps': noise_timesteps,\n",
    "    'inference_timesteps': inference_timesteps,\n",
    "    'start_beta': start_beta,\n",
    "    'end_beta': end_beta}\n",
    "\n",
    "    result_df['hyperparams'] = str(hyperparams)\n",
    "\n",
    "    filename = directory_path + diffusion_model.RECOMMENDER_NAME + \".csv\"\n",
    "\n",
    "    # Check if file exists\n",
    "    if os.path.isfile(filename):\n",
    "        # If it exists, append without writing the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        # If it doesn't exist, create it, write the header\n",
    "        pd.DataFrame(result_df.loc[cutoff]).transpose().to_csv(filename, mode='w', header=True, index=False)\n",
    "\n",
    "    return result_df.loc[cutoff][metric]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=50,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Diffusion.DiffusionRecommender import MultiBlockAttentionDiffusionRecommender\n",
    "diffusion_model = MultiBlockAttentionDiffusionRecommender(URM_train = URM_train, use_gpu = True)\n",
    "diffusion_model.fit(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, _ = evaluator_validation.evaluateRecommender(diffusion_model)\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
