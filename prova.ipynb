{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens1M: Preloaded data not found, reading from original files...\n",
      "Movielens1M: Loading original data\n",
      "Movielens1M: Unable to find data zip file. Downloading...\n",
      "Downloading: https://files.grouplens.org/datasets/movielens/ml-1m.zip\n",
      "In folder: /Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Data_manager_split_datasets/Movielens1M/ml-1m.zip\n",
      "DataReader: Downloaded 100.00%, 5.65 MB, 2545 KB/s, 2 seconds passed\n",
      "Movielens1M: Loading Interactions\n",
      "Movielens1M: Loading Item Features genres\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xe9 in position 3114: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mData_manager\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMovielens\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMovielens1MReader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Movielens1MReader\n\u001b[1;32m      9\u001b[0m data_reader \u001b[38;5;241m=\u001b[39m Movielens1MReader()\n\u001b[0;32m---> 10\u001b[0m data_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mdata_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m URM_all \u001b[38;5;241m=\u001b[39m data_loaded\u001b[38;5;241m.\u001b[39mget_URM_all()\n\u001b[1;32m     16\u001b[0m URM_train, URM_validation \u001b[38;5;241m=\u001b[39m split_train_in_two_percentage_global_sample( URM_all, train_percentage \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.8\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Data_manager/DataReader.py:146\u001b[0m, in \u001b[0;36mDataReader.load_data\u001b[0;34m(self, save_folder_path)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m: Exception while reading split\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dataset_name()))\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading original data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 146\u001b[0m loaded_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_from_original_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVerifying data consistency...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m loaded_dataset\u001b[38;5;241m.\u001b[39mverify_data_consistency()\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Data_manager/Movielens/Movielens1MReader.py:56\u001b[0m, in \u001b[0;36mMovielens1MReader._load_from_original_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m URM_all_dataframe, URM_timestamp_dataframe \u001b[38;5;241m=\u001b[39m _loadURM(URM_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Item Features genres\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m ICM_genres_dataframe, ICM_years_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43m_loadICM_genres_years\u001b[49m\u001b[43m(\u001b[49m\u001b[43mICM_genre_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m::\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenresSeparator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m|\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading User Features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     59\u001b[0m UCM_dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filepath_or_buffer\u001b[38;5;241m=\u001b[39mUCM_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m{\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m3\u001b[39m:\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;28mstr\u001b[39m}, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/TESI/Thesis_DiffusionRecommender-main/Data_manager/Movielens/_utils_movielens_parser.py:14\u001b[0m, in \u001b[0;36m_loadICM_genres_years\u001b[0;34m(genres_path, header, separator, genresSeparator)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_loadICM_genres_years\u001b[39m(genres_path, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, genresSeparator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 14\u001b[0m     ICM_genres_dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenres_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     ICM_genres_dataframe\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItemID\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenreList\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m     ICM_years_dataframe \u001b[38;5;241m=\u001b[39m ICM_genres_dataframe\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1679\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1679\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:124\u001b[0m, in \u001b[0;36mPythonParser.__init__\u001b[0;34m(self, f, **kwds)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_col_indices: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m columns: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m]]\n\u001b[1;32m    120\u001b[0m (\n\u001b[1;32m    121\u001b[0m     columns,\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_original_columns,\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols,\n\u001b[0;32m--> 124\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_infer_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Now self.columns has the set of columns that we will process.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# The original set is stored in self.original_columns.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'index_names'\u001b[39;00m\n\u001b[1;32m    129\u001b[0m (\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[1;32m    137\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:540\u001b[0m, in \u001b[0;36mPythonParser._infer_columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m         line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffered_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m names:\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:630\u001b[0m, in \u001b[0;36mPythonParser._buffered_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:731\u001b[0m, in \u001b[0;36mPythonParser._next_line\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 731\u001b[0m     orig_line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m orig_line \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:795\u001b[0m, in \u001b[0;36mPythonParser._next_iter_line\u001b[0;34m(self, row_num)\u001b[0m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[1;32m    797\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py:230\u001b[0m, in \u001b[0;36mPythonParser._make_reader.<locals>._read\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read\u001b[39m():\n\u001b[0;32m--> 230\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     pat \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(sep)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pat\u001b[38;5;241m.\u001b[39msplit(line\u001b[38;5;241m.\u001b[39mstrip())\n",
      "File \u001b[0;32m~/miniconda3/envs/Tesi/lib/python3.8/codecs.py:322\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;66;03m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m--> 322\u001b[0m     (result, consumed) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# keep undecoded input until the next call\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer \u001b[38;5;241m=\u001b[39m data[consumed:]\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xe9 in position 3114: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from Evaluation.Evaluator import EvaluatorHoldout\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "#import torch\n",
    "\n",
    "from Data_manager.split_functions.split_train_validation_random_holdout import split_train_in_two_percentage_global_sample\n",
    "from Data_manager.Movielens.Movielens1MReader import Movielens1MReader\n",
    "\n",
    "data_reader = Movielens1MReader()\n",
    "data_loaded = data_reader.load_data()\n",
    "\n",
    "URM_all = data_loaded.get_URM_all()\n",
    "\n",
    "\n",
    "\n",
    "URM_train, URM_validation = split_train_in_two_percentage_global_sample( URM_all, train_percentage = 0.8)\n",
    "\n",
    "evaluator_validation = EvaluatorHoldout(URM_validation, cutoff_list=[10])\n",
    "#evaluator_test = EvaluatorHoldout(URM_test, cutoff_list=[10])\n",
    "\n",
    "\"\"\"\n",
    "from Diffusion.DiffusionRecommender import DiffusionAutoencoderRecommender_OptimizerMask\n",
    "folder_path= '/Users/lucaortolomo/Downloads/Movielens1M/models'\n",
    "file_name='/DiffusionAutoencoderRecommender_best_model_last'\n",
    "\n",
    "diffrec = DiffusionAutoencoderRecommender_OptimizerMask(URM_train, use_gpu= False)\n",
    "diffrec.load_model(folder_path=folder_path, file_name=file_name)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<6040x3883 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 800167 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#things needed from URM\n",
    "n_users, n_items = URM_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6040"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit parameters\n",
    "epochs = 100\n",
    "batch_size = 128 #forse piu di 8(?)\n",
    "l2_reg = 1e-4 #??????\n",
    "sgd_mode ='adam'\n",
    "learning_rate = 1e-2\n",
    "noise_timesteps = 1000\n",
    "inference_timesteps = 500\n",
    "start_beta = 0.0001\n",
    "end_beta = 0.10\n",
    "activation_function = torch.nn.ReLU\n",
    "\n",
    "\n",
    "\n",
    "from Diffusion.architecture_utils import generate_autoencoder_architecture\n",
    "import numpy as np\n",
    "#specific fro autoencoder rec\n",
    "encoding_size = 50\n",
    "next_layer_size_multiplier = 2\n",
    "max_parameters = np.inf\n",
    "max_n_hidden_layers = 3\n",
    "noise_timesteps = 100\n",
    "\"\"\"\n",
    "  Example, if encoding = 50, next layer multiplier = 3, max hidden layers = 2\n",
    "    the architecture is:\n",
    "    [n_items, 450, 150, 50, 150, 450, n_items]\n",
    "    \"\"\"\n",
    "#->\n",
    "encoder_architecture = generate_autoencoder_architecture(encoding_size, n_items, next_layer_size_multiplier, max_parameters, max_n_hidden_layers)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_optimizer(optimizer_label, model, learning_rate, l2_reg):\n",
    "\n",
    "    if optimizer_label.lower() == \"adagrad\":\n",
    "        return torch.optim.Adagrad(model.parameters(), lr = learning_rate, weight_decay = l2_reg*learning_rate)\n",
    "    elif optimizer_label.lower() == \"rmsprop\":\n",
    "        return torch.optim.RMSprop(model.parameters(), lr = learning_rate, weight_decay = l2_reg*learning_rate)\n",
    "    elif optimizer_label.lower() == \"adam\":\n",
    "        return torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = l2_reg*learning_rate)\n",
    "    elif optimizer_label.lower() == \"sgd\":\n",
    "        return torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay = l2_reg*learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"sgd_mode attribute value not recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Diffusion.PositionalEncoding import SinusoidalPositionalEncoding\n",
    "from Diffusion.NoiseSchedule import LinearNoiseSchedule\n",
    "from Diffusion.DenoisingArchitectures import AutoencoderModel\n",
    "from DiffusionLuca.DiffusionModel import DiffusionModel\n",
    "#Init diffusion model\n",
    "\n",
    "#Positional encoding (not an expert at all so just going to put it there)\n",
    "positional_encoding = SinusoidalPositionalEncoding(embedding_size = n_items, device=\"cpu:0\")\n",
    "#Here i am going to use a LinearNoiseSchedule, since i understant i can rewrite it myself\n",
    "noise_schedule = LinearNoiseSchedule(start_beta=0.0001, end_beta=0.10, device = \"cpu:0\")\n",
    "\n",
    "denoiser_model = AutoencoderModel(encoder_architecture = encoder_architecture,\n",
    "                                          activation_function = activation_function,\n",
    "                                          use_batch_norm = False,\n",
    "                                          use_dropout = False,\n",
    "                                          dropout_p = 0.3,\n",
    "                                          device=\"cpu:0\")\n",
    "\n",
    "model = DiffusionModel(denoiser_model, noise_schedule, positional_encoding, noise_timesteps).to(device = \"cpu:0\")\n",
    "import scipy.sparse as sps\n",
    "warm_user_ids = np.arange(0, n_users)[np.ediff1d(sps.csr_matrix(URM_train).indptr) > 0]\n",
    "#fine init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoencoderModel(\n",
      "  (_encoder_network): Sequential(\n",
      "    (layer_0): Linear(in_features=3883, out_features=200, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=100, out_features=50, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "  )\n",
      "  (_decoder_network): Sequential(\n",
      "    (layer_0): Linear(in_features=50, out_features=100, bias=True)\n",
      "    (activation_0): ReLU()\n",
      "    (layer_1): Linear(in_features=100, out_features=200, bias=True)\n",
      "    (activation_1): ReLU()\n",
      "    (layer_2): Linear(in_features=200, out_features=3883, bias=True)\n",
      "    (activation_2): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print (model.denoiser_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start fit\n",
    "current_epoch_training_loss = 0\n",
    "_optimizer = _get_optimizer(sgd_mode.lower(), model, learning_rate, l2_reg)\n",
    "loss_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "#start training\n",
    "def training_loop(model, URM_train, warm_user_ids, n_epochs, noise_timesteps, batch_size, optimizer, show_progress_bar = True):\n",
    "    for epoch in tqdm(range(n_epochs),desc=f\"Training progress\", colour = \"#00ff00\"):\n",
    "        current_epoch_loss=0\n",
    "        model.train()\n",
    "\n",
    "        num_batches_per_epoch = math.ceil(len(warm_user_ids) / batch_size)\n",
    "        iterator = tqdm(range(num_batches_per_epoch)) if show_progress_bar else range(num_batches_per_epoch)\n",
    "#one_epoch\n",
    "        for _ in iterator:\n",
    "            user_batch = torch.LongTensor(np.random.choice(warm_user_ids, size=batch_size))\n",
    "            #print(user_batch[0])\n",
    "            # Transferring only the sparse structure to reduce the data transfer\n",
    "            user_batch_tensor = URM_train[user_batch]\n",
    "            #print(user_batch_tensor[0])\n",
    "\n",
    "            user_batch_tensor = torch.sparse_csr_tensor(user_batch_tensor.indptr,\n",
    "                                                        user_batch_tensor.indices,\n",
    "                                                        user_batch_tensor.data,\n",
    "                                                        size=user_batch_tensor.shape,\n",
    "                                                        dtype=torch.float32,\n",
    "                                                        device=\"cpu:0\",\n",
    "                                                        requires_grad=False).to_dense()\n",
    "\n",
    "            # Clear previously computed gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Sample timestamps\n",
    "            t = torch.randint(0, noise_timesteps, (len(user_batch_tensor),), device=\"cpu:0\", dtype=torch.long)\n",
    "\n",
    "            # Compute prediction for each element in batch\n",
    "            #the forward process returns the mse loss between prediction and target!!!!\n",
    "            loss = model.forward(user_batch_tensor, t)\n",
    "\n",
    "            # Compute gradients given current loss\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm(self._model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Apply gradient using the selected optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            current_epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "        print(\"Epoch {}, loss {:.2E}\".format(epoch, current_epoch_loss))\n",
    "        # self.loss_list.append(self.current_epoch_training_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_loop(model=model, URM_train=URM_train, warm_user_ids=warm_user_ids,n_epochs=100, noise_timesteps=noise_timesteps, batch_size=batch_size, optimizer=_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TopPopRecommender: URM Detected 203 ( 5.2%) items with no interactions.\n",
      "EvaluatorHoldout: Processed 6032 (100.0%) in 1.77 sec. Users per second: 3410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PRECISION                     0.182609\n",
       "PRECISION_RECALL_MIN_DEN      0.191058\n",
       "RECALL                        0.066456\n",
       "MAP                           0.112597\n",
       "MAP_MIN_DEN                   0.116327\n",
       "MRR                           0.377093\n",
       "NDCG                          0.154938\n",
       "F1                            0.097448\n",
       "HIT_RATE                      0.665451\n",
       "ARHR_ALL_HITS                 0.620683\n",
       "NOVELTY                       0.022098\n",
       "AVERAGE_POPULARITY            0.769477\n",
       "DIVERSITY_MEAN_INTER_LIST     0.494777\n",
       "DIVERSITY_HERFINDAHL           0.94947\n",
       "COVERAGE_ITEM                 0.017512\n",
       "COVERAGE_ITEM_HIT             0.017255\n",
       "ITEMS_IN_GT                   0.893381\n",
       "COVERAGE_USER                 0.998675\n",
       "COVERAGE_USER_HIT              0.66457\n",
       "USERS_IN_GT                   0.998675\n",
       "DIVERSITY_GINI                0.005007\n",
       "SHANNON_ENTROPY               4.585432\n",
       "RATIO_DIVERSITY_HERFINDAHL    0.950244\n",
       "RATIO_DIVERSITY_GINI          0.014306\n",
       "RATIO_SHANNON_ENTROPY         0.424016\n",
       "RATIO_AVERAGE_POPULARITY      2.764785\n",
       "RATIO_NOVELTY                 0.059894\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Recommenders.NonPersonalizedRecommender import TopPop\n",
    "\n",
    "toppo = TopPop(URM_train=URM_train)\n",
    "toppo.fit()\n",
    "result_df, _ = evaluator_validation.evaluateRecommender(toppo)\n",
    "result_df.loc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionAutoencoderRecommender: URM Detected 203 ( 5.2%) items with no interactions.\n",
      "DiffusionAutoencoderRecommender: Architecture: [3883, 245]\n",
      "fit!!!\n",
      "429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucaortolomo/Desktop/TESI/Thesis_DiffusionRecommender-main/Diffusion/DiffusionRecommender.py:401: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:56.)\n",
      "  user_batch_tensor = torch.sparse_csr_tensor(user_batch_tensor.indptr,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DiffusionAutoencoderRecommender: Epoch 1, loss 2.10E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 1 of 240. Elapsed time 2.35 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 2, loss 1.99E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 2 of 240. Elapsed time 4.57 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 3, loss 1.98E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 3 of 240. Elapsed time 6.76 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 4, loss 1.93E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 4 of 240. Elapsed time 8.90 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 5, loss 1.95E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 5 of 240. Elapsed time 11.01 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 6, loss 1.94E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 6 of 240. Elapsed time 13.17 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 7, loss 1.97E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 7 of 240. Elapsed time 15.32 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 8, loss 2.00E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 8 of 240. Elapsed time 17.49 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 9, loss 1.93E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 9 of 240. Elapsed time 19.66 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 10, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 10 of 240. Elapsed time 21.83 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 11, loss 1.95E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 11 of 240. Elapsed time 23.95 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 12, loss 1.94E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 12 of 240. Elapsed time 26.06 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 13, loss 1.92E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 13 of 240. Elapsed time 28.17 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 14, loss 1.95E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 14 of 240. Elapsed time 30.27 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 15, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 15 of 240. Elapsed time 32.42 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 16, loss 1.96E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 16 of 240. Elapsed time 34.58 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 17, loss 1.93E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 17 of 240. Elapsed time 36.75 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 18, loss 1.99E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 18 of 240. Elapsed time 38.91 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 19, loss 1.92E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 19 of 240. Elapsed time 41.13 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 20, loss 1.93E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 20 of 240. Elapsed time 43.27 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 21, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 21 of 240. Elapsed time 45.41 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 22, loss 1.94E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 22 of 240. Elapsed time 47.54 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 23, loss 1.96E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 23 of 240. Elapsed time 49.66 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 24, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 24 of 240. Elapsed time 51.79 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 25, loss 1.92E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 25 of 240. Elapsed time 53.93 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 26, loss 1.92E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 26 of 240. Elapsed time 56.06 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 27, loss 1.94E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 27 of 240. Elapsed time 58.17 sec\n",
      "DiffusionAutoencoderRecommender: Epoch 28, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 28 of 240. Elapsed time 1.00 min\n",
      "DiffusionAutoencoderRecommender: Epoch 29, loss 1.94E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 29 of 240. Elapsed time 1.04 min\n",
      "DiffusionAutoencoderRecommender: Epoch 30, loss 1.96E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 30 of 240. Elapsed time 1.08 min\n",
      "DiffusionAutoencoderRecommender: Epoch 31, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 31 of 240. Elapsed time 1.11 min\n",
      "DiffusionAutoencoderRecommender: Epoch 32, loss 1.95E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 32 of 240. Elapsed time 1.15 min\n",
      "DiffusionAutoencoderRecommender: Epoch 33, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 33 of 240. Elapsed time 1.18 min\n",
      "DiffusionAutoencoderRecommender: Epoch 34, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 34 of 240. Elapsed time 1.22 min\n",
      "DiffusionAutoencoderRecommender: Epoch 35, loss 1.93E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 35 of 240. Elapsed time 1.25 min\n",
      "DiffusionAutoencoderRecommender: Epoch 36, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 36 of 240. Elapsed time 1.29 min\n",
      "DiffusionAutoencoderRecommender: Epoch 37, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 37 of 240. Elapsed time 1.32 min\n",
      "DiffusionAutoencoderRecommender: Epoch 38, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 38 of 240. Elapsed time 1.36 min\n",
      "DiffusionAutoencoderRecommender: Epoch 39, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 39 of 240. Elapsed time 1.39 min\n",
      "DiffusionAutoencoderRecommender: Epoch 40, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 40 of 240. Elapsed time 1.43 min\n",
      "DiffusionAutoencoderRecommender: Epoch 41, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 41 of 240. Elapsed time 1.47 min\n",
      "DiffusionAutoencoderRecommender: Epoch 42, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 42 of 240. Elapsed time 1.50 min\n",
      "DiffusionAutoencoderRecommender: Epoch 43, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 43 of 240. Elapsed time 1.54 min\n",
      "DiffusionAutoencoderRecommender: Epoch 44, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 44 of 240. Elapsed time 1.58 min\n",
      "DiffusionAutoencoderRecommender: Epoch 45, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 45 of 240. Elapsed time 1.62 min\n",
      "DiffusionAutoencoderRecommender: Epoch 46, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 46 of 240. Elapsed time 1.66 min\n",
      "DiffusionAutoencoderRecommender: Epoch 47, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 47 of 240. Elapsed time 1.70 min\n",
      "DiffusionAutoencoderRecommender: Epoch 48, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 48 of 240. Elapsed time 1.74 min\n",
      "DiffusionAutoencoderRecommender: Epoch 49, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 49 of 240. Elapsed time 1.78 min\n",
      "DiffusionAutoencoderRecommender: Epoch 50, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 50 of 240. Elapsed time 1.82 min\n",
      "DiffusionAutoencoderRecommender: Epoch 51, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 51 of 240. Elapsed time 1.86 min\n",
      "DiffusionAutoencoderRecommender: Epoch 52, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 52 of 240. Elapsed time 1.90 min\n",
      "DiffusionAutoencoderRecommender: Epoch 53, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 53 of 240. Elapsed time 1.94 min\n",
      "DiffusionAutoencoderRecommender: Epoch 54, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 54 of 240. Elapsed time 1.98 min\n",
      "DiffusionAutoencoderRecommender: Epoch 55, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 55 of 240. Elapsed time 2.01 min\n",
      "DiffusionAutoencoderRecommender: Epoch 56, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 56 of 240. Elapsed time 2.05 min\n",
      "DiffusionAutoencoderRecommender: Epoch 57, loss 1.94E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 57 of 240. Elapsed time 2.09 min\n",
      "DiffusionAutoencoderRecommender: Epoch 58, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 58 of 240. Elapsed time 2.13 min\n",
      "DiffusionAutoencoderRecommender: Epoch 59, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 59 of 240. Elapsed time 2.17 min\n",
      "DiffusionAutoencoderRecommender: Epoch 60, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 60 of 240. Elapsed time 2.20 min\n",
      "DiffusionAutoencoderRecommender: Epoch 61, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 61 of 240. Elapsed time 2.24 min\n",
      "DiffusionAutoencoderRecommender: Epoch 62, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 62 of 240. Elapsed time 2.28 min\n",
      "DiffusionAutoencoderRecommender: Epoch 63, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 63 of 240. Elapsed time 2.32 min\n",
      "DiffusionAutoencoderRecommender: Epoch 64, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 64 of 240. Elapsed time 2.35 min\n",
      "DiffusionAutoencoderRecommender: Epoch 65, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 65 of 240. Elapsed time 2.39 min\n",
      "DiffusionAutoencoderRecommender: Epoch 66, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 66 of 240. Elapsed time 2.42 min\n",
      "DiffusionAutoencoderRecommender: Epoch 67, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 67 of 240. Elapsed time 2.46 min\n",
      "DiffusionAutoencoderRecommender: Epoch 68, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 68 of 240. Elapsed time 2.49 min\n",
      "DiffusionAutoencoderRecommender: Epoch 69, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 69 of 240. Elapsed time 2.53 min\n",
      "DiffusionAutoencoderRecommender: Epoch 70, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 70 of 240. Elapsed time 2.57 min\n",
      "DiffusionAutoencoderRecommender: Epoch 71, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 71 of 240. Elapsed time 2.60 min\n",
      "DiffusionAutoencoderRecommender: Epoch 72, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 72 of 240. Elapsed time 2.64 min\n",
      "DiffusionAutoencoderRecommender: Epoch 73, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 73 of 240. Elapsed time 2.67 min\n",
      "DiffusionAutoencoderRecommender: Epoch 74, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 74 of 240. Elapsed time 2.71 min\n",
      "DiffusionAutoencoderRecommender: Epoch 75, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 75 of 240. Elapsed time 2.74 min\n",
      "DiffusionAutoencoderRecommender: Epoch 76, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 76 of 240. Elapsed time 2.78 min\n",
      "DiffusionAutoencoderRecommender: Epoch 77, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 77 of 240. Elapsed time 2.81 min\n",
      "DiffusionAutoencoderRecommender: Epoch 78, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 78 of 240. Elapsed time 2.85 min\n",
      "DiffusionAutoencoderRecommender: Epoch 79, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 79 of 240. Elapsed time 2.88 min\n",
      "DiffusionAutoencoderRecommender: Epoch 80, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 80 of 240. Elapsed time 2.92 min\n",
      "DiffusionAutoencoderRecommender: Epoch 81, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 81 of 240. Elapsed time 2.95 min\n",
      "DiffusionAutoencoderRecommender: Epoch 82, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 82 of 240. Elapsed time 2.99 min\n",
      "DiffusionAutoencoderRecommender: Epoch 83, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 83 of 240. Elapsed time 3.02 min\n",
      "DiffusionAutoencoderRecommender: Epoch 84, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 84 of 240. Elapsed time 3.06 min\n",
      "DiffusionAutoencoderRecommender: Epoch 85, loss 1.92E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 85 of 240. Elapsed time 3.09 min\n",
      "DiffusionAutoencoderRecommender: Epoch 86, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 86 of 240. Elapsed time 3.13 min\n",
      "DiffusionAutoencoderRecommender: Epoch 87, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 87 of 240. Elapsed time 3.17 min\n",
      "DiffusionAutoencoderRecommender: Epoch 88, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 88 of 240. Elapsed time 3.21 min\n",
      "DiffusionAutoencoderRecommender: Epoch 89, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 89 of 240. Elapsed time 3.25 min\n",
      "DiffusionAutoencoderRecommender: Epoch 90, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 90 of 240. Elapsed time 3.28 min\n",
      "DiffusionAutoencoderRecommender: Epoch 91, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 91 of 240. Elapsed time 3.33 min\n",
      "DiffusionAutoencoderRecommender: Epoch 92, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 92 of 240. Elapsed time 3.36 min\n",
      "DiffusionAutoencoderRecommender: Epoch 93, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 93 of 240. Elapsed time 3.40 min\n",
      "DiffusionAutoencoderRecommender: Epoch 94, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 94 of 240. Elapsed time 3.43 min\n",
      "DiffusionAutoencoderRecommender: Epoch 95, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 95 of 240. Elapsed time 3.47 min\n",
      "DiffusionAutoencoderRecommender: Epoch 96, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 96 of 240. Elapsed time 3.51 min\n",
      "DiffusionAutoencoderRecommender: Epoch 97, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 97 of 240. Elapsed time 3.54 min\n",
      "DiffusionAutoencoderRecommender: Epoch 98, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 98 of 240. Elapsed time 3.58 min\n",
      "DiffusionAutoencoderRecommender: Epoch 99, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 99 of 240. Elapsed time 3.62 min\n",
      "DiffusionAutoencoderRecommender: Epoch 100, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 100 of 240. Elapsed time 3.65 min\n",
      "DiffusionAutoencoderRecommender: Epoch 101, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 101 of 240. Elapsed time 3.69 min\n",
      "DiffusionAutoencoderRecommender: Epoch 102, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 102 of 240. Elapsed time 3.72 min\n",
      "DiffusionAutoencoderRecommender: Epoch 103, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 103 of 240. Elapsed time 3.76 min\n",
      "DiffusionAutoencoderRecommender: Epoch 104, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 104 of 240. Elapsed time 3.80 min\n",
      "DiffusionAutoencoderRecommender: Epoch 105, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 105 of 240. Elapsed time 3.83 min\n",
      "DiffusionAutoencoderRecommender: Epoch 106, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 106 of 240. Elapsed time 3.87 min\n",
      "DiffusionAutoencoderRecommender: Epoch 107, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 107 of 240. Elapsed time 3.90 min\n",
      "DiffusionAutoencoderRecommender: Epoch 108, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 108 of 240. Elapsed time 3.94 min\n",
      "DiffusionAutoencoderRecommender: Epoch 109, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 109 of 240. Elapsed time 3.98 min\n",
      "DiffusionAutoencoderRecommender: Epoch 110, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 110 of 240. Elapsed time 4.03 min\n",
      "DiffusionAutoencoderRecommender: Epoch 111, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 111 of 240. Elapsed time 4.07 min\n",
      "DiffusionAutoencoderRecommender: Epoch 112, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 112 of 240. Elapsed time 4.11 min\n",
      "DiffusionAutoencoderRecommender: Epoch 113, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 113 of 240. Elapsed time 4.14 min\n",
      "DiffusionAutoencoderRecommender: Epoch 114, loss 1.81E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 114 of 240. Elapsed time 4.18 min\n",
      "DiffusionAutoencoderRecommender: Epoch 115, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 115 of 240. Elapsed time 4.21 min\n",
      "DiffusionAutoencoderRecommender: Epoch 116, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 116 of 240. Elapsed time 4.25 min\n",
      "DiffusionAutoencoderRecommender: Epoch 117, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 117 of 240. Elapsed time 4.29 min\n",
      "DiffusionAutoencoderRecommender: Epoch 118, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 118 of 240. Elapsed time 4.33 min\n",
      "DiffusionAutoencoderRecommender: Epoch 119, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 119 of 240. Elapsed time 4.36 min\n",
      "DiffusionAutoencoderRecommender: Epoch 120, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 120 of 240. Elapsed time 4.40 min\n",
      "DiffusionAutoencoderRecommender: Epoch 121, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 121 of 240. Elapsed time 4.43 min\n",
      "DiffusionAutoencoderRecommender: Epoch 122, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 122 of 240. Elapsed time 4.47 min\n",
      "DiffusionAutoencoderRecommender: Epoch 123, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 123 of 240. Elapsed time 4.50 min\n",
      "DiffusionAutoencoderRecommender: Epoch 124, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 124 of 240. Elapsed time 4.54 min\n",
      "DiffusionAutoencoderRecommender: Epoch 125, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 125 of 240. Elapsed time 4.58 min\n",
      "DiffusionAutoencoderRecommender: Epoch 126, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 126 of 240. Elapsed time 4.61 min\n",
      "DiffusionAutoencoderRecommender: Epoch 127, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 127 of 240. Elapsed time 4.65 min\n",
      "DiffusionAutoencoderRecommender: Epoch 128, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 128 of 240. Elapsed time 4.68 min\n",
      "DiffusionAutoencoderRecommender: Epoch 129, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 129 of 240. Elapsed time 4.72 min\n",
      "DiffusionAutoencoderRecommender: Epoch 130, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 130 of 240. Elapsed time 4.75 min\n",
      "DiffusionAutoencoderRecommender: Epoch 131, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 131 of 240. Elapsed time 4.79 min\n",
      "DiffusionAutoencoderRecommender: Epoch 132, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 132 of 240. Elapsed time 4.82 min\n",
      "DiffusionAutoencoderRecommender: Epoch 133, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 133 of 240. Elapsed time 4.86 min\n",
      "DiffusionAutoencoderRecommender: Epoch 134, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 134 of 240. Elapsed time 4.90 min\n",
      "DiffusionAutoencoderRecommender: Epoch 135, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 135 of 240. Elapsed time 4.93 min\n",
      "DiffusionAutoencoderRecommender: Epoch 136, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 136 of 240. Elapsed time 4.97 min\n",
      "DiffusionAutoencoderRecommender: Epoch 137, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 137 of 240. Elapsed time 5.00 min\n",
      "DiffusionAutoencoderRecommender: Epoch 138, loss 1.81E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 138 of 240. Elapsed time 5.04 min\n",
      "DiffusionAutoencoderRecommender: Epoch 139, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 139 of 240. Elapsed time 5.07 min\n",
      "DiffusionAutoencoderRecommender: Epoch 140, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 140 of 240. Elapsed time 5.11 min\n",
      "DiffusionAutoencoderRecommender: Epoch 141, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 141 of 240. Elapsed time 5.14 min\n",
      "DiffusionAutoencoderRecommender: Epoch 142, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 142 of 240. Elapsed time 5.18 min\n",
      "DiffusionAutoencoderRecommender: Epoch 143, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 143 of 240. Elapsed time 5.21 min\n",
      "DiffusionAutoencoderRecommender: Epoch 144, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 144 of 240. Elapsed time 5.25 min\n",
      "DiffusionAutoencoderRecommender: Epoch 145, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 145 of 240. Elapsed time 5.29 min\n",
      "DiffusionAutoencoderRecommender: Epoch 146, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 146 of 240. Elapsed time 5.32 min\n",
      "DiffusionAutoencoderRecommender: Epoch 147, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 147 of 240. Elapsed time 5.36 min\n",
      "DiffusionAutoencoderRecommender: Epoch 148, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 148 of 240. Elapsed time 5.39 min\n",
      "DiffusionAutoencoderRecommender: Epoch 149, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 149 of 240. Elapsed time 5.43 min\n",
      "DiffusionAutoencoderRecommender: Epoch 150, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 150 of 240. Elapsed time 5.46 min\n",
      "DiffusionAutoencoderRecommender: Epoch 151, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 151 of 240. Elapsed time 5.50 min\n",
      "DiffusionAutoencoderRecommender: Epoch 152, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 152 of 240. Elapsed time 5.54 min\n",
      "DiffusionAutoencoderRecommender: Epoch 153, loss 1.81E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 153 of 240. Elapsed time 5.57 min\n",
      "DiffusionAutoencoderRecommender: Epoch 154, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 154 of 240. Elapsed time 5.61 min\n",
      "DiffusionAutoencoderRecommender: Epoch 155, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 155 of 240. Elapsed time 5.64 min\n",
      "DiffusionAutoencoderRecommender: Epoch 156, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 156 of 240. Elapsed time 5.68 min\n",
      "DiffusionAutoencoderRecommender: Epoch 157, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 157 of 240. Elapsed time 5.71 min\n",
      "DiffusionAutoencoderRecommender: Epoch 158, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 158 of 240. Elapsed time 5.75 min\n",
      "DiffusionAutoencoderRecommender: Epoch 159, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 159 of 240. Elapsed time 5.78 min\n",
      "DiffusionAutoencoderRecommender: Epoch 160, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 160 of 240. Elapsed time 5.82 min\n",
      "DiffusionAutoencoderRecommender: Epoch 161, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 161 of 240. Elapsed time 5.85 min\n",
      "DiffusionAutoencoderRecommender: Epoch 162, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 162 of 240. Elapsed time 5.89 min\n",
      "DiffusionAutoencoderRecommender: Epoch 163, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 163 of 240. Elapsed time 5.93 min\n",
      "DiffusionAutoencoderRecommender: Epoch 164, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 164 of 240. Elapsed time 5.96 min\n",
      "DiffusionAutoencoderRecommender: Epoch 165, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 165 of 240. Elapsed time 6.00 min\n",
      "DiffusionAutoencoderRecommender: Epoch 166, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 166 of 240. Elapsed time 6.04 min\n",
      "DiffusionAutoencoderRecommender: Epoch 167, loss 1.81E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 167 of 240. Elapsed time 6.07 min\n",
      "DiffusionAutoencoderRecommender: Epoch 168, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 168 of 240. Elapsed time 6.11 min\n",
      "DiffusionAutoencoderRecommender: Epoch 169, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 169 of 240. Elapsed time 6.14 min\n",
      "DiffusionAutoencoderRecommender: Epoch 170, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 170 of 240. Elapsed time 6.18 min\n",
      "DiffusionAutoencoderRecommender: Epoch 171, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 171 of 240. Elapsed time 6.21 min\n",
      "DiffusionAutoencoderRecommender: Epoch 172, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 172 of 240. Elapsed time 6.25 min\n",
      "DiffusionAutoencoderRecommender: Epoch 173, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 173 of 240. Elapsed time 6.29 min\n",
      "DiffusionAutoencoderRecommender: Epoch 174, loss 1.78E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 174 of 240. Elapsed time 6.32 min\n",
      "DiffusionAutoencoderRecommender: Epoch 175, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 175 of 240. Elapsed time 6.36 min\n",
      "DiffusionAutoencoderRecommender: Epoch 176, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 176 of 240. Elapsed time 6.39 min\n",
      "DiffusionAutoencoderRecommender: Epoch 177, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 177 of 240. Elapsed time 6.43 min\n",
      "DiffusionAutoencoderRecommender: Epoch 178, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 178 of 240. Elapsed time 6.47 min\n",
      "DiffusionAutoencoderRecommender: Epoch 179, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 179 of 240. Elapsed time 6.50 min\n",
      "DiffusionAutoencoderRecommender: Epoch 180, loss 1.91E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 180 of 240. Elapsed time 6.54 min\n",
      "DiffusionAutoencoderRecommender: Epoch 181, loss 1.90E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 181 of 240. Elapsed time 6.57 min\n",
      "DiffusionAutoencoderRecommender: Epoch 182, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 182 of 240. Elapsed time 6.61 min\n",
      "DiffusionAutoencoderRecommender: Epoch 183, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 183 of 240. Elapsed time 6.65 min\n",
      "DiffusionAutoencoderRecommender: Epoch 184, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 184 of 240. Elapsed time 6.69 min\n",
      "DiffusionAutoencoderRecommender: Epoch 185, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 185 of 240. Elapsed time 6.72 min\n",
      "DiffusionAutoencoderRecommender: Epoch 186, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 186 of 240. Elapsed time 6.76 min\n",
      "DiffusionAutoencoderRecommender: Epoch 187, loss 1.81E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 187 of 240. Elapsed time 6.79 min\n",
      "DiffusionAutoencoderRecommender: Epoch 188, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 188 of 240. Elapsed time 6.83 min\n",
      "DiffusionAutoencoderRecommender: Epoch 189, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 189 of 240. Elapsed time 6.87 min\n",
      "DiffusionAutoencoderRecommender: Epoch 190, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 190 of 240. Elapsed time 6.90 min\n",
      "DiffusionAutoencoderRecommender: Epoch 191, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 191 of 240. Elapsed time 6.94 min\n",
      "DiffusionAutoencoderRecommender: Epoch 192, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 192 of 240. Elapsed time 6.98 min\n",
      "DiffusionAutoencoderRecommender: Epoch 193, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 193 of 240. Elapsed time 7.01 min\n",
      "DiffusionAutoencoderRecommender: Epoch 194, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 194 of 240. Elapsed time 7.05 min\n",
      "DiffusionAutoencoderRecommender: Epoch 195, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 195 of 240. Elapsed time 7.08 min\n",
      "DiffusionAutoencoderRecommender: Epoch 196, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 196 of 240. Elapsed time 7.12 min\n",
      "DiffusionAutoencoderRecommender: Epoch 197, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 197 of 240. Elapsed time 7.16 min\n",
      "DiffusionAutoencoderRecommender: Epoch 198, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 198 of 240. Elapsed time 7.19 min\n",
      "DiffusionAutoencoderRecommender: Epoch 199, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 199 of 240. Elapsed time 7.23 min\n",
      "DiffusionAutoencoderRecommender: Epoch 200, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 200 of 240. Elapsed time 7.27 min\n",
      "DiffusionAutoencoderRecommender: Epoch 201, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 201 of 240. Elapsed time 7.30 min\n",
      "DiffusionAutoencoderRecommender: Epoch 202, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 202 of 240. Elapsed time 7.34 min\n",
      "DiffusionAutoencoderRecommender: Epoch 203, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 203 of 240. Elapsed time 7.38 min\n",
      "DiffusionAutoencoderRecommender: Epoch 204, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 204 of 240. Elapsed time 7.41 min\n",
      "DiffusionAutoencoderRecommender: Epoch 205, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 205 of 240. Elapsed time 7.45 min\n",
      "DiffusionAutoencoderRecommender: Epoch 206, loss 1.80E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 206 of 240. Elapsed time 7.48 min\n",
      "DiffusionAutoencoderRecommender: Epoch 207, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 207 of 240. Elapsed time 7.52 min\n",
      "DiffusionAutoencoderRecommender: Epoch 208, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 208 of 240. Elapsed time 7.55 min\n",
      "DiffusionAutoencoderRecommender: Epoch 209, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 209 of 240. Elapsed time 7.59 min\n",
      "DiffusionAutoencoderRecommender: Epoch 210, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 210 of 240. Elapsed time 7.63 min\n",
      "DiffusionAutoencoderRecommender: Epoch 211, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 211 of 240. Elapsed time 7.66 min\n",
      "DiffusionAutoencoderRecommender: Epoch 212, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 212 of 240. Elapsed time 7.70 min\n",
      "DiffusionAutoencoderRecommender: Epoch 213, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 213 of 240. Elapsed time 7.73 min\n",
      "DiffusionAutoencoderRecommender: Epoch 214, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 214 of 240. Elapsed time 7.77 min\n",
      "DiffusionAutoencoderRecommender: Epoch 215, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 215 of 240. Elapsed time 7.81 min\n",
      "DiffusionAutoencoderRecommender: Epoch 216, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 216 of 240. Elapsed time 7.84 min\n",
      "DiffusionAutoencoderRecommender: Epoch 217, loss 1.87E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 217 of 240. Elapsed time 7.88 min\n",
      "DiffusionAutoencoderRecommender: Epoch 218, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 218 of 240. Elapsed time 7.91 min\n",
      "DiffusionAutoencoderRecommender: Epoch 219, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 219 of 240. Elapsed time 7.95 min\n",
      "DiffusionAutoencoderRecommender: Epoch 220, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 220 of 240. Elapsed time 7.98 min\n",
      "DiffusionAutoencoderRecommender: Epoch 221, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 221 of 240. Elapsed time 8.02 min\n",
      "DiffusionAutoencoderRecommender: Epoch 222, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 222 of 240. Elapsed time 8.06 min\n",
      "DiffusionAutoencoderRecommender: Epoch 223, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 223 of 240. Elapsed time 8.09 min\n",
      "DiffusionAutoencoderRecommender: Epoch 224, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 224 of 240. Elapsed time 8.13 min\n",
      "DiffusionAutoencoderRecommender: Epoch 225, loss 1.83E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 225 of 240. Elapsed time 8.16 min\n",
      "DiffusionAutoencoderRecommender: Epoch 226, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 226 of 240. Elapsed time 8.20 min\n",
      "DiffusionAutoencoderRecommender: Epoch 227, loss 1.88E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 227 of 240. Elapsed time 8.23 min\n",
      "DiffusionAutoencoderRecommender: Epoch 228, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 228 of 240. Elapsed time 8.27 min\n",
      "DiffusionAutoencoderRecommender: Epoch 229, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 229 of 240. Elapsed time 8.31 min\n",
      "DiffusionAutoencoderRecommender: Epoch 230, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 230 of 240. Elapsed time 8.34 min\n",
      "DiffusionAutoencoderRecommender: Epoch 231, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 231 of 240. Elapsed time 8.38 min\n",
      "DiffusionAutoencoderRecommender: Epoch 232, loss 1.85E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 232 of 240. Elapsed time 8.41 min\n",
      "DiffusionAutoencoderRecommender: Epoch 233, loss 1.86E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 233 of 240. Elapsed time 8.45 min\n",
      "DiffusionAutoencoderRecommender: Epoch 234, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 234 of 240. Elapsed time 8.48 min\n",
      "DiffusionAutoencoderRecommender: Epoch 235, loss 1.81E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 235 of 240. Elapsed time 8.52 min\n",
      "DiffusionAutoencoderRecommender: Epoch 236, loss 1.80E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 236 of 240. Elapsed time 8.55 min\n",
      "DiffusionAutoencoderRecommender: Epoch 237, loss 1.89E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 237 of 240. Elapsed time 8.59 min\n",
      "DiffusionAutoencoderRecommender: Epoch 238, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 238 of 240. Elapsed time 8.63 min\n",
      "DiffusionAutoencoderRecommender: Epoch 239, loss 1.82E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 239 of 240. Elapsed time 8.66 min\n",
      "DiffusionAutoencoderRecommender: Epoch 240, loss 1.84E+01\n",
      "DiffusionAutoencoderRecommender: Epoch 240 of 240. Elapsed time 8.70 min\n",
      "DiffusionAutoencoderRecommender: Terminating at epoch 240. Elapsed time 8.70 min\n",
      "DiffusionAutoencoderRecommender: Training complete\n",
      "EvaluatorHoldout: Processed 6032 (100.0%) in 3.06 sec. Users per second: 1972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PRECISION                     0.351177\n",
       "PRECISION_RECALL_MIN_DEN      0.373882\n",
       "RECALL                        0.156558\n",
       "MAP                           0.254568\n",
       "MAP_MIN_DEN                   0.266347\n",
       "MRR                           0.630789\n",
       "NDCG                          0.324883\n",
       "F1                            0.216568\n",
       "HIT_RATE                      0.886936\n",
       "ARHR_ALL_HITS                 1.202685\n",
       "NOVELTY                       0.024015\n",
       "AVERAGE_POPULARITY            0.507474\n",
       "DIVERSITY_MEAN_INTER_LIST     0.929306\n",
       "DIVERSITY_HERFINDAHL          0.992915\n",
       "COVERAGE_ITEM                 0.212207\n",
       "COVERAGE_ITEM_HIT             0.185939\n",
       "ITEMS_IN_GT                   0.893381\n",
       "COVERAGE_USER                 0.998675\n",
       "COVERAGE_USER_HIT             0.885762\n",
       "USERS_IN_GT                   0.998675\n",
       "DIVERSITY_GINI                0.049236\n",
       "SHANNON_ENTROPY               7.931511\n",
       "RATIO_DIVERSITY_HERFINDAHL    0.993725\n",
       "RATIO_DIVERSITY_GINI          0.140673\n",
       "RATIO_SHANNON_ENTROPY         0.733429\n",
       "RATIO_AVERAGE_POPULARITY      1.823391\n",
       "RATIO_NOVELTY                 0.065088\n",
       "Name: 10, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Diffusion.DiffusionRecommender import DiffusionAutoencoderRecommender_OptimizerMask\n",
    "\n",
    "modelP = DiffusionAutoencoderRecommender_OptimizerMask(URM_train, use_gpu= False)\n",
    "modelP.fit(epochs= 240, learning_rate= 0.00022443058798716912, l2_reg= 1.9501901504153858e-05, batch_size= 128, start_beta= 0.00038565911164407826, end_beta= 0.12444974448281955, noise_timesteps= 429.80432866068713, inference_timesteps= 1, objective= 'pred_x0', sgd_mode= 'rmsprop', activation_function= 'GELU', encoding_size= 245, next_layer_size_multiplier= 5, max_n_hidden_layers= 5, max_parameters= 1000000000.0)\n",
    "result_df, _ = evaluator_validation.evaluateRecommender(modelP)\n",
    "result_df.loc[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (2884270800.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    result_df.loc[10]\u001b[0m\n\u001b[0m                     \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from Recommenders.Neural.MultVAERecommender import MultVAERecommender_OptimizerMask\n",
    "\n",
    "compare = MultVAERecommender_OptimizerMask(URM_train=URM_train)\n",
    "compare.fit(epochs=200)\n",
    "\n",
    "result_df, _ = evaluator_validation.evaluateRecommender(compare)\n",
    "result_df.loc[10]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
